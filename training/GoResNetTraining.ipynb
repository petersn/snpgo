{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob, random, time, os, zlib\n",
    "\n",
    "FEATURE_COUNT = 16 + 6\n",
    "CROSS_VAL_SIZE = 1000\n",
    "MINIBATCH_SIZE = 128\n",
    "DEVICE_TO_USE = \"/gpu:0\"\n",
    "DATA_ROOT = \"/home/snp/proj/go/fastgo/massive_small_chunks\"\n",
    "TOTAL_CHUNK_COUNT = 39 # Intentionally low to leave some test data.\n",
    "\n",
    "def to_hms(x):\n",
    "    x = int(x)\n",
    "    seconds = x % 60\n",
    "    minutes = (x // 60) % 60\n",
    "    hours   = x // 60 // 60\n",
    "    return \"%2i:%02i:%02i\" % (hours, minutes, seconds)\n",
    "\n",
    "def stream_decompress(s):\n",
    "    decomp = zlib.decompressobj()\n",
    "    block_size = 2**23\n",
    "    i = 0\n",
    "    results = []\n",
    "    while i < len(s):\n",
    "        block = s[i:i+block_size]\n",
    "        results.append(decomp.decompress(block))\n",
    "        i += block_size\n",
    "    results.append(decomp.flush())\n",
    "    return \"\".join(results)\n",
    "\n",
    "def load_chunk(features, targets, winners):\n",
    "    def load_flat_array(path, shape):\n",
    "        with open(path) as f:\n",
    "            data = f.read()\n",
    "        data = stream_decompress(data)\n",
    "        return np.fromstring(data, dtype=np.int8).reshape(shape)\n",
    "    features = load_flat_array(features, (-1, FEATURE_COUNT, 19, 19))\n",
    "    targets  = load_flat_array(targets, (-1, 19, 19))\n",
    "    winners  = load_flat_array(winners, (-1, 2))\n",
    "    assert len(features) == len(targets) == len(winners)\n",
    "    # Take the features, and move the feature index to the end, so it has shape (-1, 19, 19, FEATURE_COUNT)\n",
    "    features = np.moveaxis(features, 1, -1)\n",
    "    assert features.shape == (len(features), 19, 19, FEATURE_COUNT)\n",
    "    return {\"features\": features, \"targets\": targets, \"winners\": winners}\n",
    "\n",
    "def load_chunk_from_fastgo_chunks_by_index(chunk_index):\n",
    "    return load_chunk(\n",
    "        os.path.join(DATA_ROOT, \"features_%03i.z\" % chunk_index),\n",
    "        os.path.join(DATA_ROOT, \"targets_%03i.z\" % chunk_index),\n",
    "        os.path.join(DATA_ROOT, \"winners_%03i.z\" % chunk_index),\n",
    "    )\n",
    "\n",
    "# Views into the extremely large dataset.\n",
    "next_chunk_index = 0\n",
    "chunk = None\n",
    "cross_val = None\n",
    "training_samples = None\n",
    "in_sample_test = None\n",
    "\n",
    "def chunk_index_by(index):\n",
    "    result = chunk[\"features\"][index], chunk[\"targets\"][index]\n",
    "    assert len(result[0]) == len(result[1]), \"Different length inputs and outputs! This should never happen!\"\n",
    "    return result\n",
    "\n",
    "def load_next_chunk():\n",
    "    global next_chunk_index, chunk, cross_val, training_samples, in_sample_test\n",
    "    print(\"    >>> Loading chunk:\", next_chunk_index)\n",
    "    # Free the memory from the previous chunk FIRST, if we have one loaded.\n",
    "    # This is necessary to avoid running out of memory.\n",
    "    if chunk is not None:\n",
    "        del chunk\n",
    "        #del cross_val\n",
    "        del training_samples\n",
    "        del in_sample_test\n",
    "    start = time.time()\n",
    "    chunk = load_chunk_from_fastgo_chunks_by_index(next_chunk_index)\n",
    "    next_chunk_index = (next_chunk_index + 1) % total_chunk_count\n",
    "    #cross_val = chunk_index_by(slice(None, CROSS_VAL_SIZE))\n",
    "    cvs = 0\n",
    "    training_samples = chunk_index_by(slice(cvs, None))\n",
    "    in_sample_test   = chunk_index_by(slice(cvs, cvs + 1000))\n",
    "    stop = time.time()\n",
    "    print(\"    >>> (In %f) Samples: %i\" % (stop - start, len(chunk[\"features\"])))\n",
    "\n",
    "def apply_symmetry(feature_arr, target_arr):\n",
    "    assert feature_arr.shape == (19, 19, FEATURE_COUNT)\n",
    "    assert target_arr.shape == (19, 19)\n",
    "    # Break views.\n",
    "    feature_arr = np.array(feature_arr)\n",
    "    target_arr = np.array(target_arr)\n",
    "    coin = lambda: random.choice((False, True))\n",
    "    if coin():\n",
    "        feature_arr = feature_arr[::-1,:,:]\n",
    "        target_arr  = target_arr [::-1,:]\n",
    "    if coin():\n",
    "        feature_arr = feature_arr[:,::-1,:]\n",
    "        target_arr  = target_arr [:,::-1]\n",
    "    if coin():\n",
    "        feature_arr = np.swapaxes(feature_arr, 0, 1)\n",
    "        target_arr  = np.swapaxes(target_arr,  0, 1)\n",
    "    assert feature_arr.shape == (19, 19, FEATURE_COUNT)\n",
    "    assert target_arr.shape == (19, 19)\n",
    "    return feature_arr, target_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('    >>> Loading chunk:', 0)\n",
      "    >>> (In 14.552898) Samples: 847083\n"
     ]
    }
   ],
   "source": [
    "# Load up a forever-fixed cross-val set.\n",
    "load_next_chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123456789)\n",
    "cross_val_data = load_chunk(\n",
    "    os.path.join(DATA_ROOT, \"features_039.z\"),\n",
    "    os.path.join(DATA_ROOT, \"targets_039.z\"),\n",
    "    os.path.join(DATA_ROOT, \"winners_039.z\"),\n",
    ")\n",
    "# Choose random indices.\n",
    "indices = random.sample(list(range(len(cross_val_data[\"features\"]))), CROSS_VAL_SIZE)\n",
    "cross_val = np.array([cross_val_data[\"features\"][i] for i in indices]), \\\n",
    "            np.array([cross_val_data[\"targets\"][i] for i in indices])\n",
    "# Delete the original data.\n",
    "del cross_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('stddev:', 0.10050378152592121)\n",
      "('stddev:', 0.034020690871988585)\n",
      "('stddev:', 0.034020690871988585)\n",
      "('stddev:', 0.034020690871988585)\n",
      "('stddev:', 0.034020690871988585)\n",
      "('stddev:', 0.034020690871988585)\n",
      "('stddev:', 0.034020690871988585)\n",
      "('stddev:', 0.034020690871988585)\n",
      "('stddev:', 0.034020690871988585)\n",
      "('stddev:', 0.034020690871988585)\n",
      "('stddev:', 0.034020690871988585)\n",
      "('stddev:', 0.034020690871988585)\n",
      "('stddev:', 0.034020690871988585)\n",
      "('stddev:', 0.034020690871988585)\n",
      "('stddev:', 0.034020690871988585)\n",
      "('stddev:', 0.034020690871988585)\n",
      "('stddev:', 0.034020690871988585)\n",
      "('stddev:', 0.10206207261596575)\n",
      "WARNING:tensorflow:From <ipython-input-4-074d766e27e0>:52: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "product = lambda l: reduce(lambda x, y: x * y, l, 1)\n",
    "total_parameters = 0\n",
    "\n",
    "def weight_variable(shape):\n",
    "    global total_parameters\n",
    "    total_parameters += product(shape)\n",
    "    stddev = (2.0 / product(shape[:-1]))**0.5\n",
    "    initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    global total_parameters\n",
    "    total_parameters += product(shape)\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "\n",
    "class GoResNet:\n",
    "    INPUT_FEATURE_COUNT = 22\n",
    "    OUTPUT_SOFTMAX_COUNT = 361\n",
    "    FILTERS = 192\n",
    "    CONV_SIZE = 3\n",
    "    NONLINEARITY = [tf.nn.relu]\n",
    "    BLOCK_COUNT = 8\n",
    "\n",
    "    def __init__(self):\n",
    "        # Construct input/output placeholders.\n",
    "        self.input_ph = tf.placeholder(\n",
    "            tf.float32,\n",
    "            shape=[None, 19, 19, self.INPUT_FEATURE_COUNT],\n",
    "            name=\"input_placeholder\")\n",
    "        self.desired_output_ph = tf.placeholder(\n",
    "            tf.float32,\n",
    "            shape=[None, 19, 19],\n",
    "            name=\"desired_output_placeholder\")\n",
    "        self.learning_rate_ph = tf.placeholder(tf.float32, shape=[], name=\"learning_rate\")\n",
    "        self.is_training_ph = tf.placeholder(tf.bool, shape=[], name=\"is_training\")\n",
    "\n",
    "        # Begin constructing the data flow.\n",
    "        self.parameters = []\n",
    "        self.flow = self.input_ph\n",
    "        # Stack an initial convolution.\n",
    "        self.stack_convolution(3, self.INPUT_FEATURE_COUNT, self.FILTERS)\n",
    "        self.stack_nonlinearity()\n",
    "        # Stack some number of residual blocks.\n",
    "        for _ in xrange(self.BLOCK_COUNT):\n",
    "            self.stack_block()\n",
    "        # Stack a final batch-unnormalized 1x1 convolution.\n",
    "        self.stack_convolution(1, self.FILTERS, 1, batch_normalization=False)\n",
    "\n",
    "        # Construct the training components.\n",
    "        self.flattened = tf.reshape(self.flow, [-1, self.OUTPUT_SOFTMAX_COUNT])\n",
    "        self.flattened_desired_output = tf.reshape(self.desired_output_ph, [-1, self.OUTPUT_SOFTMAX_COUNT])\n",
    "        self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=self.flattened_desired_output,\n",
    "            logits=self.flattened,\n",
    "        ))\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=0.0001)\n",
    "        reg_variables = tf.trainable_variables()\n",
    "        self.regularization_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\n",
    "        self.loss = self.cross_entropy + self.regularization_term\n",
    "\n",
    "        # Associate batch normalization with training.\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.train_step = tf.train.MomentumOptimizer(\n",
    "                learning_rate=self.learning_rate_ph, momentum=0.9).minimize(self.loss)\n",
    "\n",
    "    def stack_convolution(self, kernel_size, old_size, new_size, batch_normalization=True):\n",
    "        weights = weight_variable([kernel_size, kernel_size, old_size, new_size])\n",
    "        self.parameters.append(weights)\n",
    "        self.flow = conv2d(self.flow, weights)\n",
    "        if batch_normalization:\n",
    "            self.flow = tf.layers.batch_normalization(\n",
    "                self.flow,\n",
    "                center=False,\n",
    "                scale=False,\n",
    "                training=self.is_training_ph)\n",
    "        else:\n",
    "            bias = bias_variable([new_size])\n",
    "            self.parameters.append(bias)\n",
    "            self.flow = self.flow + bias # TODO: Is += equivalent?\n",
    "\n",
    "    def stack_nonlinearity(self):\n",
    "        self.flow = self.NONLINEARITY[0](self.flow)\n",
    "\n",
    "    def stack_block(self):\n",
    "        initial_value = self.flow\n",
    "        # Stack the first convolution.\n",
    "        self.stack_convolution(3, self.FILTERS, self.FILTERS)\n",
    "        self.stack_nonlinearity()\n",
    "        # Stack the second convolution.\n",
    "        self.stack_convolution(3, self.FILTERS, self.FILTERS)\n",
    "        # Add the skip connection.\n",
    "        self.flow = self.flow + initial_value\n",
    "        # Stack on the deferred non-linearity.\n",
    "        self.stack_nonlinearity()\n",
    "\n",
    "    def train(self, samples, learning_rate):\n",
    "        self.run_on_samples(self.train_step.run, samples, learning_rate=learning_rate, is_training=True)\n",
    "\n",
    "    def get_loss(self, samples):\n",
    "        return self.run_on_samples(self.cross_entropy.eval, samples)\n",
    "\n",
    "    def get_accuracy(self, samples):\n",
    "        results = self.run_on_samples(self.flattened.eval, samples)\n",
    "        results = np.argmax(results, axis=1)\n",
    "        assert results.shape == (len(samples[0]),)\n",
    "        correct = 0\n",
    "        for sample, result in zip(samples[1], results):\n",
    "            correct += np.argmax(sample) == result\n",
    "        return correct / float(len(samples[0]))\n",
    "\n",
    "    def run_on_samples(self, f, samples, learning_rate=0.01, is_training=False):\n",
    "        input_tensor, output_tensor = samples\n",
    "        return f(feed_dict={\n",
    "            self.input_ph:          input_tensor,\n",
    "            self.desired_output_ph: output_tensor,\n",
    "            self.learning_rate_ph:  learning_rate,\n",
    "            self.is_training_ph:    is_training,\n",
    "        })\n",
    "\n",
    "# We now build our network.\n",
    "with tf.device(DEVICE_TO_USE):\n",
    "    cnn = GoResNet()\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "print \"Total parameters:\", total_parameters\n",
    "\n",
    "total_training_steps = 0\n",
    "loss_plot = []\n",
    "in_sample_loss_plot = []\n",
    "lr = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_counter = 0\n",
    "def save_model():\n",
    "    global model_save_counter\n",
    "    model_save_counter += 1\n",
    "    x_conv_weights = [(sess.run(W), sess.run(b)) for W, b in cnn.convolution_weights]\n",
    "    path = \"MASSIVE-resnet-%03i\" % (model_save_counter,)\n",
    "    np.save(path, [x_conv_weights])\n",
    "    print \"\\x1b[35mSaved model to:\\x1b[0m\", path"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Load up weights from disk and set the network parameters from them.\n",
    "#loaded_conv_weights, = np.load(\"gen10-2.npy\")\n",
    "#loaded_conv_weights, = np.load(\"LONG-saved-model-017.npy\")\n",
    "#loaded_conv_weights, = np.load(\"AWS2-saved-model-039.npy\")\n",
    "#loaded_conv_weights, = np.load(\"AWS5-saved-model-040.npy\")\n",
    "loaded_conv_weights, = np.load(\"AWS4-saved-model-094.npy\")\n",
    "for new_pair, var_pair in zip(loaded_conv_weights, cnn.convolution_weights):\n",
    "    for value, var in zip(new_pair, var_pair):\n",
    "        sess.run(var.assign(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m   334 [ 0:00:00 -  0:00:00] Loss: 3.284615  In-sample loss: 3.407727  Accuracy: 28.200\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bd7954103feb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mminibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mworking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mtotal_work\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mworking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Try really hard to not keep any views around!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-074d766e27e0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, samples, learning_rate)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_on_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-074d766e27e0>\u001b[0m in \u001b[0;36mrun_on_samples\u001b[0;34m(self, f, samples, learning_rate, is_training)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesired_output_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate_ph\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_training_ph\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         })\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   2211\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m     \"\"\"\n\u001b[0;32m-> 2213\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m \u001b[0m_gradient_registry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegistry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gradient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4813\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4814\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4815\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_work = 0.0\n",
    "start_time = time.time()\n",
    "best_loss = float(\"inf\")\n",
    "bad = 0\n",
    "\n",
    "lr_schedule = lambda step: 0.005 * 0.5**(step / 10e4)\n",
    "\n",
    "for overall_step in range(10000):\n",
    "    elapsed = time.time() - start_time\n",
    "    in_sample_loss = cnn.get_loss(in_sample_test)\n",
    "    loss = cnn.get_loss(cross_val)\n",
    "    color_pair = \"\", \"\"\n",
    "    if loss < best_loss:\n",
    "        color_pair = \"\\x1b[31m\", \"\\x1b[0m\"\n",
    "    print(\"%s%6i [%s - %s] Loss: %.6f  In-sample loss: %.6f  Accuracy: %.3f%s\" % (\n",
    "        color_pair[0],\n",
    "        total_training_steps,\n",
    "        to_hms(elapsed),\n",
    "        to_hms(total_work),\n",
    "        loss,\n",
    "        in_sample_loss,\n",
    "        cnn.get_accuracy(cross_val) * 100,\n",
    "        color_pair[1]\n",
    "    ))\n",
    "    loss_plot.append((total_training_steps, loss))\n",
    "    in_sample_loss_plot.append((total_training_steps, in_sample_loss))\n",
    "\n",
    "    if loss >= best_loss:\n",
    "        bad_threshold = 3 #5 + 3\n",
    "        if lr >= 0.001:\n",
    "            bad_threshold = 2\n",
    "        if lr >= 0.003:\n",
    "            bad_threshold = 1\n",
    "        bad += 1\n",
    "        if bad >= bad_threshold:\n",
    "            lr *= 0.75 #0.75\n",
    "            lr = max(lr, lr_schedule(total_training_steps))\n",
    "            print \"\\x1b[33mLearning rate reduction!\\x1b[0m lr =\", lr\n",
    "            bad = 0\n",
    "    else:\n",
    "        bad = 0\n",
    "    best_loss = min(best_loss, loss)\n",
    "\n",
    "    for _ in range(500):\n",
    "        indices = []\n",
    "        while len(indices) < MINIBATCH_SIZE:\n",
    "            i = random.randrange(len(training_samples[0]))\n",
    "            if np.any(training_samples[1][i]):\n",
    "                indices.append(i)\n",
    "        #indices = random.sample(xrange(len(training_samples[0])), MINIBATCH_SIZE)\n",
    "        features = []\n",
    "        targets  = []\n",
    "        for i in indices:\n",
    "            feat_arr = training_samples[0][i]\n",
    "            targ_arr = training_samples[1][i]\n",
    "            feat_arr, targ_arr = apply_symmetry(feat_arr, targ_arr)\n",
    "            features.append(feat_arr)\n",
    "            targets.append(targ_arr)\n",
    "            del feat_arr\n",
    "            del targ_arr\n",
    "        #features = np.array([training_samples[0][i] for i in indices])\n",
    "        #targets  = np.array([training_samples[1][i] for i in indices])\n",
    "        minibatch = (features, targets)\n",
    "        working = time.time()\n",
    "        cnn.train(minibatch, lr)\n",
    "        total_work += time.time() - working\n",
    "        # Try really hard to not keep any views around!\n",
    "        del minibatch\n",
    "        del features\n",
    "        del targets\n",
    "        total_training_steps += 1\n",
    "\n",
    "    # Periodically swap out the data for fresh training data.\n",
    "    if (overall_step + 1) % 5 == 0:\n",
    "        load_next_chunk()\n",
    "    if (overall_step + 1) % 20 == 0:\n",
    "        save_model()\n",
    "\n",
    "print(\"%6i [%s] Accuracy: %7.3f%%\" % (total_training_steps, to_hms(elapsed), 100.0 * cnn.get_accuracy(cross_val)))\n",
    "#plt.hold(True)\n",
    "#plt.plot(*zip(*loss_plot[1:]))\n",
    "#plt.plot(*zip(*in_sample_loss_plot[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everything from here on down is horrific cruft to be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*zip(*loss_plot[1:]))\n",
    "plt.plot(*zip(*in_sample_loss_plot[1:]))\n",
    "cnn.get_accuracy(cross_val) * 100"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "move_index = 6\n",
    "f = cross_val[0][move_index]\n",
    "for i in xrange(20):\n",
    "    plt.matshow(f[:,:,i])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "move_index = 19\n",
    "case = [cross_val[0][move_index]], [cross_val[1][move_index]]\n",
    "\n",
    "output = cnn.run_on_samples(cnn.y_conv.eval, case)\n",
    "output = output.reshape((19, 19))\n",
    "output = np.exp(output)\n",
    "black = case[0][0][:,:,2]\n",
    "white = case[0][0][:,:,3]\n",
    "plt.matshow(black - white) #+ (output / np.max(output) * 0.4) + case[1].reshape((19, 19))*2)\n",
    "plt.matshow(output)\n",
    "plt.matshow(case[1][0].reshape((19, 19)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "move_index = 11 #3 + 34\n",
    "true_index = move_index / 2 #1 + 10\n",
    "\n",
    "#def rotate(array):\n",
    "#    assert array.shape == (19, 19)\n",
    "#    return array.T[::-1,:]\n",
    "\n",
    "#def inverse_rotate(array):\n",
    "#    assert array.shape == (19, 19)\n",
    "#    return array.T[:,::-1]\n",
    "\n",
    "# Produce the rotated version.\n",
    "#thing = np.array(final_validation[0][move_index])\n",
    "#assert thing.shape == (19, 19, 22)\n",
    "#for i in xrange(22):\n",
    "#    thing[:,:,i] = inverse_rotate(thing[:,:,i])\n",
    "\n",
    "case = [final_validation[0][move_index]], [final_validation[1][move_index]]\n",
    "#case = [thing], [final_validation[1][move_index]]\n",
    "\n",
    "output = cnn.run_on_samples(cnn.y_conv.eval, case)\n",
    "output = output.reshape((19, 19))\n",
    "output = np.exp(output)\n",
    "#output = rotate(output)\n",
    "black = case[0][0][:,:,2]\n",
    "white = case[0][0][:,:,3]\n",
    "\n",
    "#assert np.all(case[0][0] == all_ENGINE_features[true_index])\n",
    "#assert np.all(np.exp(all_ENGINE_heatmaps[true_index].reshape((19, 19))) == output)\n",
    "\n",
    "# Extract some features.\n",
    "#for i in xrange(22):\n",
    "    #assert np.all(inverse_rotate(case[0][0][:,:,i]) == all_ENGINE_features[1 + 10][:,:,i])\n",
    "    #assert np.all(case[0][0][:,:,i] == all_ENGINE_features[1 + 10][:,:,i])\n",
    "#plt.matshow(case[0][0][:,:,1])\n",
    "#plt.matshow(all_ENGINE_features[1 + 10][:,:,1])\n",
    "\n",
    "# Show the board state.\n",
    "plt.matshow(black - white) #+ (output / np.max(output) * 0.4) + case[1].reshape((19, 19))*2)\n",
    "# Show the trained model computed heatmap.\n",
    "plt.matshow(output)\n",
    "# Show the heatmap that was produced.\n",
    "#plt.matshow(np.exp(all_ENGINE_heatmaps[1 + 10].reshape((19, 19))))\n",
    "# Show the move that was selected.\n",
    "plt.matshow(case[1][0].reshape((19, 19)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
